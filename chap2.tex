\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=1.0in, top=1.0in, bottom=1.0in, right=1.0in]{geometry}
\usepackage[hidelinks]{hyperref}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{bbm, bm}                     % blackboard and bold fonts
\usepackage{mathtools}                   % provides starred version of bmatrix. loads amsmath
\usepackage{float}                       % more exact placement of figures
\usepackage{array}                       % justification of fixed-width columns & matrices
\usepackage{arydshln}                    % dashed lines in matrices
\usepackage{titlesec}                    % change title formats
\usepackage[most, breakable]{tcolorbox}  % nicely colored boxes (most allows for no frames)
\usepackage[shortlabels]{enumitem}       % change enumerate symbols
\usepackage{nicefrac}                    % small in-line fractions
\usepackage{booktabs}                    % nice table properties, e.g. toprule & bottomrule
\usepackage{wasysym}                     % larger set of glyphs for, e.g. binary relations
\usepackage[bottom]{footmisc}            % put footnote in environment at bottom of page
\usepackage{threeparttable}              % notes at the table
% \usepackage[capposition=top]{floatrow}   % `floatfoot` command for figure notes

\usepackage{tikz}       % drawing stuff
\usetikzlibrary{matrix} % drawing matrices

\usepackage{stackengine, scalerel} % adding some padding in fractions

\usepackage{siunitx}
\sisetup{
    per-mode = symbol,
    output-decimal-marker = {.},
    % group-minimum-digits = 4,
    input-symbols = (),
    range-units = brackets,
    list-final-separator = { \translate{and} },
    list-pair-separator = { \translate{and} },
    range-phrase = { \translate{to (numerical range)} },
}

%-- enumerate options
\setlist{
    listparindent=\parindent,
    parsep=0pt
}


% trinidad colors
\definecolor{tgreen}{RGB}{57, 115, 82}
\definecolor{tlightgreen}{RGB}{121, 217, 163}
\definecolor{tmarigold}{RGB}{242, 159, 5}
\definecolor{torange}{RGB}{217, 103, 4}
\definecolor{tred}{RGB}{115, 32, 2}
\definecolor{tcoral}{RGB}{242, 121, 131}
\definecolor{tblue}{RGB}{12, 99, 102}
\definecolor{tlightblue}{RGB}{20, 161, 166}
\definecolor{tlava}{RGB}{73, 65, 55}
\definecolor{tcoffee}{RGB}{58, 48, 45}



\titleformat{\section}
    {\large\scshape}{\thesection}{1em}{}
\titleformat{\subsection}
    {\large\scshape}{\thesubsection}{1em}{}

% allows equation and align environments to break over pages
\allowdisplaybreaks

% faster way of writing a norm
\newcommand\norm[1]{\left\lVert#1\right\rVert}

%vectors with angle, parenthesis, & bracket delimiters
\newcommand\avec[1]{\left\langle#1\right\rangle}
\newcommand\pvec[1]{\left(#1\right)}
\newcommand\bvec[1]{\left[#1\right]}

% argmin and argmax operators
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

% shortening varepsilon
\newcommand\ve{\varepsilon}

% shortening common bold-faced letters and variance
\newcommand\E{\mathbb{E}}
\newcommand\R{\mathbb{R}}
\newcommand\Var{\text{Var}}
\newcommand\Cov{\text{Cov}}
\renewcommand\P{\mathbb{P}} % \P is the paragraph symbol by default

\newcommand\cF{\mathcal{F}}
\newcommand\cG{\mathcal{G}}

% succsim & precsim with slashes through it
%   https://tex.stackexchange.com/a/479194
\makeatletter
\newcommand\nsuccsim{\mathrel{\mathpalette\varn@t\succsim}}
\newcommand\nprecsim{\mathrel{\mathpalette\varn@t\precsim}}
\newcommand\varn@t[2]{%
    \vphantom{/{#2}}%
    \ooalign{\hfil$\m@th#1/\mkern2mu$\cr\hfil$\m@th#1#2$\hfil\cr}%
}
\makeatother

% partial derivatives
\newcommand\dd[2]{\frac{\partial#1}{\partial#2}}
\newcommand\ndd[2]{\nicefrac{\partial#1}{\partial#2}}

\stackMath
\newcommand\tdd[3][1pt]{\tfrac{%
    \ThisStyle{\addstackgap[#1]{\SavedStyle\partial#2}}}{%
    \ThisStyle{\addstackgap[#1]{\SavedStyle\partial#3}}%
}}

% spaced fraction
\stackMath
\newcommand\sfrac[3][1pt]{\tfrac{%
    \ThisStyle{\addstackgap[#1]{\SavedStyle#2}}}{%
    \ThisStyle{\addstackgap[#1]{\SavedStyle#3}}%
}}

% decreases space between tilde and character its above
\newcommand\stilde[1]{\smash{\tilde{#1}}}

% label one equation in align
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

% making smaller bullets in itemize environments
\newlength{\mylen}
\setbox1=\hbox{$\bullet$}\setbox2=\hbox{\tiny$\bullet$}
\setlength{\mylen}{\dimexpr0.5\ht1-0.5\ht2}

% fixed-width, aligned column types
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}

% environments for questions and answers
\newcounter{question}[section]
\NewDocumentEnvironment{hwquestion}{o}
    {
        \refstepcounter{question}
        \begin{tcolorbox}[
            colback=tlightgreen,
            colframe=tgreen,
            opacityback=0.25,
            title=\IfNoValueTF{#1}
                {Question~\thequestion}
                {Question~\thequestion \, - #1},
            breakable,
            enhanced jigsaw,
            before upper={\parindent15pt} %-- add paragraph indents
        ]
    }
    {
        \end{tcolorbox}
    }
\newcounter{subquestion}[question]
\NewDocumentEnvironment{hwsubquestion}{o}
    {
        \refstepcounter{subquestion}
        \begin{tcolorbox}[
            colback=tlightgreen,
            colframe=tgreen,
            opacityback=0.25,
            title=\IfNoValueTF{#1}
                {Question~\thequestion.\thesubquestion}
                {Question~\thequestion.\thesubquestion \, - #1},
            breakable,
            enhanced jigsaw,
            before upper={\parindent15pt} %-- add paragraph indents
        ]
    }
    {
        \end{tcolorbox}
    }
\newenvironment{hwanswer}
    {
        \vspace{2mm}
        {\bfseries Answer}
        \vspace{-\abovedisplayskip}
        \begin{center}
            \begin{tcolorbox}[
                width=0.95\textwidth,
                colback=white,
                colframe=white,
                opacityback=0,
                opacityframe=0,
                boxrule=0pt,
                frame hidden,
                breakable,
                before upper={\parindent15pt} %-- add paragraph indents
            ]
            \lineskip=0pt % smashes inline math to same line spacing
    }
    {
        \end{tcolorbox}
        \end{center}
        \vspace{4mm}
    }
\newenvironment{hwquestiongroup}[1][\unskip]
    {
        \begin{center}
            \begin{tcolorbox}[
                width=0.90\linewidth,
                colback=torange,
                colframe=torange,
                opacityback=0.25,
                title= #1,
                breakable,
                enhanced jigsaw
            ]
    }
    {
            \end{tcolorbox}
        \end{center}
    }
\newenvironment{hwlemma}[1][\unskip]
    {
        \vspace{1mm}
        \begin{center}
            \begin{tcolorbox}[
                width=0.95\linewidth,
                colback=tmarigold,
                colframe=tmarigold,
                opacityback=0.10,
                title= #1,
                breakable,
                enhanced jigsaw
            ]
    }
    {
            \end{tcolorbox}
        \end{center}
    }
\newenvironment{hwdefinition}[1][\unskip]
    {
        \vspace{1mm}
        \begin{center}
            \begin{tcolorbox}[
                width=0.95\linewidth,
                colback=tcoral,
                colframe=tcoral,
                opacityback=0.10,
                title= #1,
                breakable,
                enhanced jigsaw
            ]
    }
    {
            \end{tcolorbox}
        \end{center}
    }

% environment for header that includes question numbers
\newenvironment{hwheader}
    {
        \begin{flushright}
            \begin{tcolorbox}[
                width=0.55\textwidth,
                colback=tlightblue,
                colframe=tblue,
                opacityback=0.25,
                enhanced jigsaw
            ]
                \begin{flushright}
                    Logan Hotz \\
    }
    {
                \end{flushright}
            \end{tcolorbox}
        \end{flushright}
        \vspace{4mm}
    }





\begin{document}



    \begin{hwheader}
        Stochastic Calculus for Finance II

        Shreve
    \end{hwheader}





    %-- question 1
    \begin{hwquestion}
        Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a general probability space, and suppose
        a random variable $X$ on this space is measurable with respect to the trivial
        $\sigma$-algebra $\mathcal{F}_0 = \{ \varnothing, \Omega \}$. Show that $X$ is not
        random (i.e., there is a constant $c$ such that $X(\omega) = c$ for all $\omega \in
        \Omega$). Such a random variable is called \emph{degenerate}.
    \end{hwquestion}

    \begin{hwanswer}
        % \vspace{2mm}
        A random variable $X$ is said to be measurable with respect to a $\sigma$-algebra
        $\mathcal{G}$ if $\{ \omega \in \Omega \, | \, X(\omega) \in B \} \subseteq
        \mathcal{G}$ for all Borel subsets of the range of $X$, $B$. Let $X$ be a random
        variable measurable with respect to $\mathcal{F}_0 = \{ \varnothing, \Omega \}$ and
        suppose there exists $\omega_1, \omega_2 \in \Omega$ for which $X(\omega_1) \neq
        X(\omega_2)$. Taking the Borel set $B_1 = \{ X(\omega_1) \}$, it must be the case
        that $\{ X \in B_1 \}$ is not empty, and because $X$ is measurable with respect to
        $\mathcal{F}_0$, we are left to conclude $\{ X \in B_1 \} = \Omega$. This is a
        contradiction, though, as $X(\omega_2) \notin B_1$ implies $\omega_2 \notin \{ X
        \in B_1 \} = \Omega$. Thus, for all $\omega \in \Omega$, it must be the case that
        $X(\omega) = c$ for some $c$ in the image of $X$.

        \vspace{5mm}

        If one assumes $X$ takes values in the reals and has an associated law, an
        alternative proof could look like the following.

        A random variable $X$ is said to be measurable with respect to a $\sigma$-algebra
        $\mathcal{G}$ if $\{ \omega \in \Omega \, | \, X(\omega) \in B \} \subseteq
        \mathcal{G}$ for all Borel subsets $B$. For all $x \in \mathbb{R}$ (and therefore
        all Borel sets of the form $(-\infty, x)$), then, $\{ X \leq x \} \in \mathcal{F}_0
        \equiv \{ \varnothing, \Omega \}$. Hence, $\P(X \leq x)$ takes one of two values:
        \[
            \begin{aligned}
                \P(X \leq x) = 0
                &\quad \text{ if } \quad
                \{ X \leq x \}
                =
                \{ \omega \in \Omega\, | \, X(\omega) \leq x \} = \varnothing \\
                \P(X \leq x) = 1
                &\quad \text{ if } \quad
                \{ X \leq x \}
                =
                \{ \omega \in \Omega\, | \, X(\omega) \leq x \} = \Omega
            \end{aligned}
        \]
        Because $\lim_{x \to -\infty} \P(X \leq x) = 0$ and $\lim_{x \to \infty} \P(X \leq
        x) = 1$, there exists a $c \in \mathbb{R}$ for which $\P(X \leq c) = 1$ and $\P(X
        \leq x) = 0$ for all $x < c$. Thus
        \[
            \P(X = c)
            =
            \lim_{\ve \to 0} \P(c - \ve < X \leq c)
            =
            \P(X \leq c) - \lim_{\ve \to 0} \P(X \leq c - \ve)
            =
            1 - 0
            =
            1.
        \]
    \end{hwanswer}





    %-- question 2
    \begin{hwquestion}
        Independence of random variables can be affectedby changes of measure. To illustrate
        this point, consider the space of two coin tosses $\Omega_2 = \{ HH, HT, TH, TT \}$,
        and let stock prices be given by
        \[
            \begin{aligned}
                S_0 &= 4
                %
                &
                S_1(H) &= 8
                %
                &
                S_2(HH) &= 16 \\
                %
                %
                {} & {}
                %
                &
                S_1(T) &= 2
                %
                &
                S_2(HT) &= S_2(TH) = 4 \\
                %
                %
                {} & {}
                %
                &
                {} & {}
                %
                &
                S_2(TT) &= 1.
            \end{aligned}
        \]
        Consider two probability measures given by
        \[
            \begin{aligned}
                \widetilde{\P}(HH) &= \nicefrac{1}{4}
                %
                &
                \widetilde{\P}(HT) &= \nicefrac{1}{4}
                %
                &
                \widetilde{\P}(TH) &= \nicefrac{1}{4}
                %
                &
                \widetilde{\P}(TT) &= \nicefrac{1}{4} \\
                %
                %
                \P(HH) &= \nicefrac{4}{9}
                %
                &
                \P(HT) &= \nicefrac{2}{9}
                %
                &
                \P(TH) &= \nicefrac{2}{9}
                %
                &
                \P(TT) &= \nicefrac{1}{9}.
            \end{aligned}
        \]
        Define the random variable
        \[
            X = \begin{cases}
                1 &\text{ if } S_2 = 4 \\
                0 &\text{ if } S_2 \neq 4
            \end{cases}
        \]

        \begin{enumerate}[(a), nolistsep]
            \item List all the sets in $\sigma(X)$.
            \item List all the sets in $\sigma(S_1)$.
            \item Show that $\sigma(X)$ and $\sigma(S_1)$ are independent under the
            probability measure $\widetilde{\P}$.
            \item Show that $\sigma(X)$ and $\sigma(S_1)$ are not independent under the
            probability measure $\P$.
            \item Under $\P$, we have $\P(S_1 = 8) = \nicefrac{2}{3}$ and $\P(S_1 = 2) =
            \nicefrac{1}{3}$. Explain intuitively why, if you are told that $X = 1$, you
            would want to revise your estimate of the distribution of $S_1$.
        \end{enumerate}
    \end{hwquestion}

    \begin{hwanswer}
        \begin{enumerate}[(a)]
            \item The $\sigma$-algebra generated by $X$ is
            $
                \sigma(X)
                =
                \big\{
                    \varnothing,
                    \Omega,
                    \{ HH, TT \},
                    \{ HT, TH \}
                \big\}.
            $

            \item The $\sigma$-algebra generated by $S_1$ is
            $
                \sigma(S_1)
                =
                \big\{
                    \varnothing,
                    \Omega,
                    \{ HH, HT \},
                    \{ TH, TT \}
                \big\}.
            $

            \item Note that it is sufficient to only consider the non-trivial events (i.e.
            those which are not $\varnothing$ nor $\Omega$). For those non-trivial events
            in $\sigma(X)$, we have
            \[
                \widetilde{\P}(\{HH, TT\})
                =
                \widetilde{\P}(HH) + \widetilde{\P}(TT)
                =
                \sfrac{1}{4} + \sfrac{1}{4}
                =
                \sfrac{1}{2}
                \quad \text{ and } \quad
                \widetilde{\P}(\{HT, TH\})
                =
                \sfrac{1}{2}.
            \]
            Similarly, both non-trivial events in $\sigma(S_1)$ also occur with probability
            $\nicefrac{1}{2}$. Hence for those non-trivial $A \in \sigma(X), B \in \sigma(
            S_1)$, we have $\widetilde{\P}(A)\widetilde{\P}(B) = (\nicefrac{1}{2})
            (\nicefrac{1}{4}) = \nicefrac{1}{4}$.

            In the tables below we show the joint events and their probabilities,
            respectively:
            \begin{table}[H]
                \centering
                \begin{tabular}{l c c}
                    \toprule
                    $\{A \cap B\}$ & $\{ HH, HT \}$ & $\{ TH, TT \}$ \\
                    \midrule
                    $\{ HH, TT \}$ & $HH$ & $TT$ \\
                    $\{ HT, HT \}$ & $HT$ & $TH$ \\
                    \bottomrule
                \end{tabular}%
                \hspace{1em}
                \begin{tabular}{l c c}
                    \toprule
                    $\widetilde{\P}(A \cap B)$ & $\{ HH, HT \}$ & $\{ TH, TT \}$ \\
                    \midrule
                    $\{ HH, TT \}$ & $\nicefrac{1}{4}$ & $\nicefrac{1}{4}$ \\
                    $\{ HT, HT \}$ & $\nicefrac{1}{4}$ & $\nicefrac{1}{4}$ \\
                    \bottomrule
                \end{tabular}
            \end{table}
            \noindent Since $\widetilde{\P}(A \cap B) = \widetilde{\P}(A) \widetilde{\P}(B)$
            for all $A \in \sigma(A), B \in \sigma(S_1)$, those $\sigma$-algebras are
            independent under $\widetilde{\P}$.

            \item The probabilities of the non-trivial events in $\sigma(X)$ are
            \[
                \P(\{HH, TT\})
                =
                \P(HH) + \P(TT)
                =
                \sfrac{4}{9} + \sfrac{1}{9}
                =
                \sfrac{5}{9}
                \quad \text{ and } \quad
                \P(\{HT, TH\})
                =
                \sfrac{4}{9},
            \]
            whereas those in $\sigma(S_1)$ are $\P(\{HH, HT\}) = \nicefrac{2}{3}$ and $\P(
            \{TH, TT\}) = \nicefrac{1}{3}$. Below we use the same table setup as in part (c)
            to show the probabilities of the joint events:
            \begin{table}[H]
                \centering
                \begin{tabular}{l c c}
                    \toprule
                    $\{A \cap B\}$ & $\{ HH, HT \}$ & $\{ TH, TT \}$ \\
                    \midrule
                    $\{ HH, TT \}$ & $HH$ & $TT$ \\
                    $\{ HT, HT \}$ & $HT$ & $TH$ \\
                    \bottomrule
                \end{tabular}%
                \hspace{1em}
                \begin{tabular}{l c c}
                    \toprule
                    $\widetilde{\P}(A \cap B)$ & $\{ HH, HT \}$ & $\{ TH, TT \}$ \\
                    \midrule
                    $\{ HH, TT \}$ & $\nicefrac{4}{9}$ & $\nicefrac{2}{9}$ \\
                    $\{ HT, HT \}$ & $\nicefrac{2}{9}$ & $\nicefrac{1}{9}$ \\
                    \bottomrule
                \end{tabular}
            \end{table}
            \noindent Take, for example, $A = \{ HH, TT \} \in \sigma(X)$ and $B = \{
            HH, HT \} \in \sigma(S_1)$. We have
            \[
                \sfrac{4}{9}
                =
                \P(\{ A \cap B \})
                \neq
                \P(A)\P(B)
                =
                \left( \sfrac{5}{9} \right)
                \left( \sfrac{2}{3} \right)
                =
                \sfrac{10}{27}.
            \]
            Thus $\sigma(X)$ and $\sigma(S_1)$ are not independent under $\P$.

            The work done here was a little excessive, of course. We really just needed
            one joint event $A \cap B, A \in \sigma(X), B \in \sigma(S_1)$, to show that
            the two $\sigma$-algebras are not independent.

            \item Having been told $X = 1$, we know that one of the events in $\{TH, HT\}$
            occured. These events are equally likely under $\P$, so with the information
            that it's a $50$-$50$ chance the first coin flip is tails, one might want to
            shift more of the estimated mass of $S_1$'s distribution to that outcome. This
            is all downstream of our conclusion in part (d): $X$ and $S_1$ are not
            independent under $\P$, so information about one random variable gives us
            information about the other.
        \end{enumerate}
    \end{hwanswer}





    %-- question 3
    \begin{hwquestion}[Rotating the Axes]
        Let $X$ and $Y$ be independent standard normal random variables. Let $\theta$ be a
        constant, and define the random variables
        \[
            V = X \cos \theta + Y \sin \theta
            \quad \text{ and } \quad
            W = -X\sin \theta + Y \cos \theta.
        \]
        Show that $V$ and $W$ are independent standard normal random variables.
    \end{hwquestion}

    \begin{hwanswer}
        Note that $\cos \theta$ and $\sin \theta$ are simply constants, as $\theta$ is
        assumed to be one. Hence the mean of $V$ is $\E V = \cos \theta \E X + \sin \theta
        \E Y = 0 + 0$ by the linearity of the expectations operator. The variance of $V$
        is $\Var V = \cos^2 \theta \Var X + \sin^2 \theta \Var Y = (\sin^2 \theta + \cos^2
        \theta) (1) = 1$. Finally, as $V$ is a linear combination of independent normals, it
        is also a normally distributed random variable. For the same reasons, $W$ is a
        mean-zero normal random variable with unit variance.

        It remains to be shown that $\Cov(V, W) = \E[VW] = 0$, which is a sufficient
        condition for the normal random variables $V$ and $W$ to be independent. We have
        \begin{multline*}
            VW
            =
            (Y^2 - X^2) \sin \theta \cos \theta
            +
            XY (\cos^2 \theta - \sin^2 \theta) \\
            %
            %
            \implies
            \E\big[ VW \big]
            =
            (\Var Y - \Var X) \sin\theta\cos\theta
            +
            \E X \E Y (\cos^2 \theta - \sin^2 \theta)
            =
            0,
        \end{multline*}
        where the first equality in the second line uses the assumed independence of $X$
        and $Y$.
    \end{hwanswer}





    %-- question 4
    \begin{hwquestion}
        In Example 2.2.8, $X$ is a standard normal random variable and $Z$ is an independent
        random variable satisfying
        \[
            \P(Z = 1) = \P(Z = -1) = \sfrac{1}{2}.
        \]
        We defined $Y = XZ$ and showed that $Y$ is standard normal. We established that
        although $X$ and $Y$ are uncorrelated, they are not independent. In this exercise
        we use moment-generating functions to show that $Y$ is standard normal and $X$ and
        $Y$ are not independent.

        \begin{enumerate}[(i), nolistsep]
            \item Establish the joint moment-generating function formula
            \[
                \E\big[ e^{uX + vY} \big]
                =
                e^{\frac{1}{2} (u^2 + v^2)}
                \frac{e^{uv} + e^{-uv}}{2}.
            \]
            \item Use the formula above to show that $\E [e^{vY} ] = e^{\frac{1}{2}v^2}$.
            This is the moment-generating function for a standard normal random variable,
            and thus $Y$ must be a standard normal random variable.

            \item Use the formula in (i) and Theorem 2.2.7(iv) to show that $X$ and $Y$ are
            not independent.
        \end{enumerate}
    \end{hwquestion}

    \begin{hwanswer}
        \begin{enumerate}[(i)]
            \item From the Law of Total Expectation, we have $\E[e^{uX + vY}] = \E[e^{uX +
            vY} \, | \, Z = 1] \P(Z = 1) + \E[e^{uX + vY} \, | \, Z = -1] \P(Z = -1)$.
            Substituting $Y = XZ$ into these expressions, we have
            \[
                \E\big[ e^{uX + vY} \big]
                =
                \sfrac{1}{2}
                \E\big[ e^{(u + v)X} \big]
                +
                \sfrac{1}{2}
                \E\big[ e^{(u - v)X} \big].
            \]
            The moment-generating function of a standard normal random variable $X$ is
            $\E[e^{sX}] = e^{\frac{1}{2}s^2}$. Evaluating this at $s = u \pm v$, we have
            the desired result:
            \[
                \E\big[ e^{uX + vY} \big]
                =
                \sfrac{1}{2}
                e^{\frac{1}{2}(u+v)^2}
                +
                \sfrac{1}{2}
                e^{\frac{1}{2}(u-v)^2}
                =
                e^{\frac{1}{2}(u^2 + v^2)}
                \frac{
                    e^{uv} + e^{-uv}
                }{
                    2
                }.
            \]

            \item Evaluate the joint moment-generating function from (i) at $u = 0$:
            \[
                \E\big[ e^{vY} \big]
                =
                \E\big[e^{(0)X + vY}\big]
                =
                e^{\frac{1}{2}((0)^2 + v^2)}
                \frac{
                    e^{(0)v} + e^{-(0)v}
                }{
                    2
                }
                =
                e^{\frac{1}{2}v^2}.
            \]

            \item Theorem 2.2.7 states, among other things, that random variables $X$ and
            $Y$ are independent if and only if their moment-generating functions factor as
            $\E[e^{uX + vY}] = \E[e^{uX}] \E[e^{vY}]$. Factoring a portion of $\E[e^{uX +
            vY}]$ to resemble $\E[e^{vY}]$, it is clear the remainder is \emph{not} $\E[e^{
            uX}]$:
            \[
                \E[e^{uX + vY}]
                =
                e^{\frac{1}{2} v^2}
                e^{\frac{1}{2} u^2}
                \frac{
                    e^{uv} + e^{-uv}
                }{
                    2
                }
                =
                \E[e^{vY}]
                \underbrace{
                    e^{\frac{1}{2} u^2}
                    \frac{
                        e^{uv} + e^{-uv}
                    }{
                        2
                    }
                }_{\neq \E[e^{uX}]}.
            \]
            Thus, we conclude $X$ and $Y$ are not independent.
        \end{enumerate}
    \end{hwanswer}

    



    %-- question 5
    \begin{hwquestion}
        Let $(X, Y)$ be a pair of random variables with joint density function
        \[
            f_{X,Y}(x, y)
            =
            \begin{cases}
                \frac{2|x|+y}{\sqrt{2\pi}}
                \exp\left\{
                    -\frac{(2|x|+y)^2}{2}
                \right\}
                &
                \text{if } y \geq -|x| \\
                0
                &
                \text{if } y < -|x|.
            \end{cases}
        \]
        Show that $X$ and $Y$ are standard normal random variables and that they are
        uncorrelated but not independent.
    \end{hwquestion}

    \begin{hwanswer}
        We recover the density of $X$ by integrating out $Y$. First, consider
        \[
            f_X(x)
            =
            \int
            f_{X,Y}(x,y)
            \,
            \text{d} y
            =
            \int_{-|x|}^{\infty}
            \sfrac{
                2|x|+y
            }{
                \sqrt{2\pi}
            }
            \exp\left\{
                -
                \sfrac{
                    (2|x|+y)^2
                }{
                    2
                }
            \right\}
            \,
            \text{d} y
            =
            \sfrac{1}{\sqrt{2\pi}}
            \int_{|x|}^{\infty}
            u
            e^{-\frac{u^2}{2}}
            \,
            \text{d} u.
        \]
        The last equality above follows from the substitution $u = 2|x| + y$. This
        integral evaluates to
        \[
            f_X(x)
            =
            - \sfrac{1}{\sqrt{2\pi}}e^{-v} \bigg|_{\frac{1}{2}|x|^2}^{\infty}
            =
            \sfrac{1}{\sqrt{2\pi}}
            e^{-\frac{1}{2}x^2},
        \]
        which is the density of a standard normal random variable.

        The process for recovering the marginal density of $Y$ is the same, although a
        little more involved. Let $\bm{1}_{A}$ be the indicator function for the statement
        $A$, so that
        \[
            f_Y(y)
            =
            \int f_{X, Y}(x, y) \, \text{d} x
            =
            \int
            \bm{1}_{y \leq |x|}
            \sfrac{2|x|+y}{\sqrt{2\pi}}
            \exp\left\{
                -\sfrac{(2|x|+y)^2}{2}
            \right\}
            \, \text{d} x.
        \]
        We split this integral into two parts, based on the sign of $y$ relative to zero:
        \[
            f_Y(y)
            =
            \int_{-|x|}
        \]


    \end{hwanswer}





    %-- question 6
    \begin{hwquestion}
        Consider a probability space $\Omega$ with four elements, which we call $a, b, c,$
        and $d$ (i.e. $\Omega = \{ a, b, c, d \}$). The $\sigma$-algebra $\cF$  is the
        collection of all subsets of $\Omega$; i.e. $\cF = 2^{\Omega}$. We define a
        probability measure $\P$ by specifying that
        \[
            \P(a) = \sfrac{1}{6},
            \quad
            \P(b) = \sfrac{1}{3},
            \quad
            \P(c) = \sfrac{1}{4},
            \quad
            \P(d) = \sfrac{1}{4},
        \]
        and, as usual, the probability of every other set in $\cF$ is the sum of the
        probabilities of the elements in the set, e.g. $\P(a, b, c) = \P(a) + \P(b) + \P(c)
        = \nicefrac{3}{4}$.

        We next define two random variables, $X$ and $Y$, by the formulas
        \[
            \begin{aligned}
                X(a) &= 1, &X(b) &=  1, &X(c) &= -1, &X(d) &= -1, \\
                Y(a) &= 1, &Y(b) &= -1, &Y(c) &=  1, &Y(d) &= -1. \\
            \end{aligned}
        \]
        We then define $Z = X + Y$.

        \begin{enumerate}[(i), nolistsep]
            \item List the sets in $\sigma(X)$.
            \item Determine $\E[Y \, | \, X]$ (i.e. specify the values of this random
            variable for $a, b, c$, and $d$). Verify the partial-averaging property is
            satisfied.
            \item Determine $\E[Z \, | \, X]$. Again verify the partial-averaging property.
            \item Compute $\E[Z \, | \, X] - \E[Y \, | \, X]$. Citing the appropriate
            properties of conditional expectation from Theorem 2.3.2, explain why you get
            $X$.
        \end{enumerate}
    \end{hwquestion}

    \begin{hwanswer}
        \begin{enumerate}[(i)]
            \item The $\sigma$-algebra generated by $X$ is
            $
                \sigma(X)
                =
                \big\{
                    \varnothing, \Omega, \{ a, b \}, \{ c, d \}
                \big\}.
            $

            \item Given the $\sigma$-algebra specified in (i), evaluating the expectation
            of $Y$ conditional on $a$ or $b$ is equivalent to $\E[ Y \, | \, X = 1 ]$;
            it is the same for $\{ c, d \}$ and $\E[ Y \, | \, X = -1 ]$. Hence,
            \[
                \E \big[ Y \, | \, X = 1 \big]
                =
                \sfrac{1}{\P(X = 1)}
                \left[
                    (1) \P(a)
                    +
                    (-1) \P(b)
                \right]
                =
                \sfrac{1}{\nicefrac{1}{2}}
                \left[
                    \sfrac{1}{6}
                    -
                    \sfrac{1}{3}
                \right]
                =
                -
                \sfrac{1}{3}
            \]
            and
            \[
                \E \big[ Y \, | \, X = -1 \big]
                =
                \sfrac{1}{\P(X = -1)}
                \left[
                    (1) \P(c)
                    +
                    (-1) \P(d)
                \right]
                =
                \sfrac{1}{\nicefrac{1}{2}}
                \left[
                    \sfrac{1}{4}
                    -
                    \sfrac{1}{4}
                \right]
                =
                0.
            \]
            The law of total expectation implies
            \[
                \begin{aligned}
                    \E[Y]
                        &=
                    \E[Y \, | \, X = 1] \P(X = 1)
                    +
                    \E[Y \, | \, X = -1] \P(X = -1)
                    \\
                    %
                    %
                    &=
                    \left(-\sfrac{1}{3}\right) \left(\sfrac{1}{2}\right)
                    +
                    (0) \left(\sfrac{1}{2}\right)
                    =
                    -\sfrac{1}{2}.
                \end{aligned}
            \]
            On the other hand, by direct evaluation of $\E[Y]$ we have
            \[
                \E[Y]
                =
                (1) \P(a) + (-1) \P(b) + (1) \P(c) + (-1) \P(d)
                =
                (1) \sfrac{1}{6} + (-1) \sfrac{1}{3} = -\sfrac{1}{2}.
            \]

            \item From the linearity of the expectations operator and the fact that $X$ is
            measurable with respect to $\sigma(X)$, we have
            \[
                \E\big[ Z \, | \, X \big]
                =
                \E\big[ Y + X \, | |, X \big]
                =
                \E\big[ Y \, | \, X \big]
                +
                \E\big[ X \, | \, X \big]
                =
                \E\big[ Y \, | \, X \big]
                +
                X.
            \]
            Hence,
            \[
                \E\big[ Z \, | \, X = 1 \big]
                =
                \E\big[ Y \, | \, X = 1 \big]
                +
                (1)
                =
                \sfrac{2}{3}
            \]
            and
            \[
                \E\big[ Z \, | \, X = -1 \big]
                =
                \E\big[ Y \, | \, X = -1 \big]
                +
                (-1)
                =
                -1.
            \]
            By the law of total expectation and direct computation, respectively, we have
            \begin{minipage}{\linewidth}
                \centering
                {\Huge UNFINISHED}
            \end{minipage}
        \end{enumerate}
    \end{hwanswer}





    %-- question 7
    \begin{hwquestion}
        Let $Y$ be an integrable random variable on a probability space $(\Omega, \cF, \P)$
        and let $\cG$ eb a sub-$\sigma$-algebra of $\cF$. Based on the information in $\cG$,
        we can form the estimate $\E[Y | \cG]$ of $Y$ and define the error of the
        estimation $\text{Err} = Y - \E[Y | \cG]$. This is a random variable with
        expectation zero and some variance $\Var(\text{Err})$. Let $X$ be some other
        $\cG$-measurable random variable, which we can regard as another estimate of $Y$.
        Show that
        \[
            \Var(\text{Err}) \leq \Var(Y - X).
        \]
        In other words, the estimate $\E[Y | \cG]$ minimizes the variance of the error
        among all estimates based on the information in $\cG$.
    \end{hwquestion}

    \begin{hwanswer}
        Define the constant $\mu = \E[Y - X)$, and note that the variance of $Y - X$ is
        then written as $\Var(Y - X) = \E\big[ (Y - X - \mu)^2 \big]$. Judiciously
        inserting a $\E[Y | \cG] - \E[Y | \cG]$ term, we further have
        \[
            \E\big[ (Y - X - \mu)^2 \big]
            =
            \E\left[
                \big( (Y - \E[Y | \cG]) + (\E[Y | \cG] - X - \mu) \big)^2
            \right].
        \]
        Multiplying this out, we have
        \[
            \begin{multlined}
                \Var(Y - X)
                =
                \E\big[ (Y - X - \mu)^2 \big]
                \\
                %
                %
                =
                \E\big[ (\text{Err} - 0)^2 \big]
                +
                \E\big[ (\E[Y | \cG] - X - \mu)^2 \big]
                +
                2 \E\big[ \text{Err} (\E[Y | \cG] - X - \mu) \big]
            \end{multlined}
        \]
        Since the $\text{Err}$ random variable has zero mean, the first term on the
        right-hand side above is $\Var(\text{Err})$. The second term is weakly greater
        than zero, and we will show below that the last term is exactly zero.

        Towards that end, we condition an inner expectation operator on $\cG$ to arrive
        at
        \[
            \begin{aligned}
                \E\big[
                    \text{Err}(\E[Y | \cG] - X - \mu)
                \big]
                &=
                \E\left[
                    \E\big[
                        \text{Err}(\E[Y | \cG] - X - \mu)
                        |
                        \cG
                    \big]
                \right]
                \\
                %
                %
                &=
                \E\left[
                    \E\big[ \text{Err} | \cG \big]
                    (\E[Y | \cG] - X - \mu)
                \right].
            \end{aligned}
        \]
        The final equality follows from the $\cG$-measurability of $X$, the fact that
        $\E[Y | \cG]$ is known conditioned on $\cG$, and that $\mu$ is a constant. Turning
        to $\E[\text{Err} | \cG]$, we immediately have that
        \[
            \E\big[ \text{Err} | \cG \big]
            =
            \E\big[ Y - \E[Y | \cG] \, | \, \cG \big]
            =
            \E[Y | \cG]
            -
            \E[Y | \cG]
            = 0.
        \]
        Thus, the cross-term $\E[\text{Err}(\E[Y | \cG] - X - \mu)]$ evaluates to zero. This
        implies that
        \[
            \Var(Y - X)
            =
            \Var(\text{Err})
            +
            \underbrace{
                \E\big[ (\E[Y | \cG] - X - \mu)^2 \big]
            }_{\geq 0}
            + 0.
        \]
        Because the second term is weakly greater than zero, we can conclude that
        \[
            \Var(\text{Err}) \geq \Var(Y - X),
        \]
        and we are done.
    \end{hwanswer}





    %-- question 8
    \begin{hwquestion}
        Let $X$ and $Y$ be integrable random variables on a probability space $(\Omega,
        \cF, \P)$. Then $Y = Y_1 + Y_2$, where $Y_1 = \E[Y | X]$ is $\sigma(X)$-measurable
        and $Y_2 = Y - \E[Y | X]$. Show that $Y_2$ and $X$ are uncorrelated. More generally,
        show that $Y_2$ is uncorrelated with every $\sigma(X)$-measurable random variable.
    \end{hwquestion}





    %-- question 9
    \begin{hwquestion}
        Let $X$ be a random variable.

        \vspace{2mm}

        \begin{enumerate}[(i), nolistsep]
            \item Given an example of a probability space $(\Omega, \cF, \P)$, a random
            variable $X$ defined on this probability space, and a function $f$ so that the
            $\sigma$-algebra generated by $f(X)$ is not the trivial $\sigma$-algebra $\{
            \varnothing, \Omega \}$ but is strictly smaller than the $\sigma$-algebra
            generated by $X$.

            \item Can the $\sigma$-algebra generated by $f(X)$ ever be strictly larger than
            the $\sigma$-algebra generated by $X$?
        \end{enumerate}
    \end{hwquestion}





    %-- question 10
    \begin{hwquestion}
        Let $X$ and $Y$ be random variables (on some unspecified probability space $(\Omega,
        \cF, \P)$), assume they have a joint density $f_{X,Y}(x,y)$, and assume $\E[|Y|] <
        \infty$. In particular, for every Borel subset $C$ of $\R^2$, we have
        \[
            \P\big( (X, Y) \in C \big)
            =
            \int_{C} f_{X,Y}(x, y) dx dy.
        \]

        In elementary probability, one learns to compute $\E[Y | X = x]$, which is a
        \emph{nonrandom} function of the \emph{dummy variable} $x$, by the formula
        \[
            \E[Y \, | \, X = x]
            =
            \int_{-\infty}^{\infty}
            y
            f_{Y | X}(y | x)
            \, \text{d} y,
        \]
        where $f_{Y|X}(y|x)$ is the \emph{conditional density} defined by
        \[
            f_{Y|X}(y|x)
            =
            \frac{f_{X,Y}(x, y)}{f_{X}(x)}.
        \]
        The denominator in this expression, $f_{X}(x) = \int_{-\infty}^{\infty} f_{X,Y}(
        x, \eta) \, \text{d} \eta$, is the \emph{marginal density} of $X$, and we must
        assume it is strictly positive for every $x$. We introduce the symbol $g(x)$ for the
        function $\E[Y | X = x]$ defined above; i.e.,
        \[
            g(x)
            =
            \int_{-\infty}^{\infty}
            y
            f_{Y | X}(y | x)
            \, \text{d} y
            =
            \int_{-\infty}^{\infty}
            \frac{
                y f_{X,Y}(x, y)
            }{
                f_X(x)
            }
            \, \text{d} y.
        \]

        In measure-theoretic probability, conditional expectation is a \emph{random
        variable} $\E[Y | X]$. This exercise is to show that when there is a joint density
        for $(X, Y)$, this random variable can be obtained by substituting the random
        variable $X$ in place of the dummy variable $x$ in the function $g(x)$. In other
        words, this exercise is to show that
        \[
            \E[Y | X]
            =
            g(X).
        \]
        (We introduced the symbol $g(x)$ in order to avoid the mathematically confusing
        expression $\E[Y | X = X]$.)

        Since $g(X)$ is obviously $\sigma(X)$-measurable, to verify that $\E[Y|X] = g(X)$,
        we need only check that the partial-averaging property is satisfied. For every
        Borel-measurable function $h$ mapping $\R$ to $\R$ and satisfying $\E[|h(X)|] <
        \infty$, we have
        \[
            \E[ h(X) ]
            =
            \int_{-\infty}^{\infty}
            h(x) f_X(x)
            \, \text{d} x.
        \]
        This is Theorem 1.5.2 in Chapter 1. Similarly, if $h$ is a function of both $x$ and
        $y$, then
        \[
            \E[ h(X, Y) ]
            =
            \int_{-\infty}^{\infty}
            \int_{-\infty}^{\infty}
            h(x, y)
            f_{X,Y}(x, y) \, \text{d} x \text{d} y
        \]
        whenever $(X, Y)$ has a joint density $f_{X, Y}(x, y)$. You may use both
        expressions, $\E[h(X)]$ and $\E[h(X, Y)]$, in your solution to this problem.

        Let $A$ be a set in $\sigma(X)$. By the definition of $\sigma(X)$, there is a Borel
        subset $B$ of $\R$ such that $A = \{ \omega \in \Omega \, | \, X(\omega) \in B \}$
        or, more simply, $A = \{ X \in B \}$. Show the partial-averaging property
        \[
            \int_{A} g(X) \, \text{d} \P
            =
            \int_{A} Y \, \text{d} \P.
        \]
    \end{hwquestion}





    %-- question 11
    \begin{hwquestion}
        \begin{enumerate}[(i), nolistsep]
            \item Let $X$ be a random variable on a probability space $(\Omega, \cF, \P)$,
            and let $W$ be a nonnegative $\sigma(X)$-measurable random variable. Show there
            exists a function $g$ such that $W = g(X)$.

            \item Let $X$ be a random variable on a probability space $(\Omega, \cF, \P)$,
            and let $Y$ be a nonnegative random variable on this space. We do not assume
            that $X$ and $Y$ have a joint density. Nonetheless, show there is a function $g$
            such that $\E[Y | X] = g(X)$.
        \end{enumerate}
    \end{hwquestion}






\end{document}
